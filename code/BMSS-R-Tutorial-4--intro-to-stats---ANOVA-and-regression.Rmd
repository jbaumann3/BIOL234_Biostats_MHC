---
title: 'BMSS R Tutorial 4: Intro to stats- ANOVA and regression'
author: "Justin Baumann"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    df_print: kable
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
  word_document:
    toc: yes
editor_options:
  chunk_output_type: console
---
# **LEARNING OBJECTIVES**
IN THIS TUTORIAL YOU WILL LEARN: </br>
1.) The theory behind a T-test and how to perform one
2.) The theory behind analysis of variance and how to perform it
3.) How to test model/ statistical assumptions (and why)
4.) How to perform a linear regression

***

# Additional Tutorials and Resources for basic statistics in R  

[t-tests](https://statistics.berkeley.edu/computing/r-t-tests)

[More advanced info on t-tests](http://www.sthda.com/english/wiki/unpaired-two-samples-t-test-in-r)

[one-way ANOVA](http://www.sthda.com/english/wiki/one-way-anova-test-in-r)

[Advanced ANOVA](https://www.datanovia.com/en/lessons/anova-in-r/)

[For the bold: Linear Models (an intro)](https://ourcodingclub.github.io/tutorials/modelling/)

[For the bold part II: Mixed effects models](https://ourcodingclub.github.io/tutorials/mixed-models/)

***

# **1.) Load packages we need**

Here we are loading our usual packages plus some additional ones for statistics. 
Performance is very useful as an assumption checker, so we will rely heavily on it. I'll also show you how to assess outliers using rstatix. 

```{r}
library(tidyverse) #For data manipulation and graphing
library(ggsci) #for color scales
library(patchwork)  #to panel our graphs
library(performance) #check stat/model assumptions
library(see)
library(rstatix) #test for outliers
```

# **2.) A note on statistics and experimental design**

Statistics is a complex field with a long history. We could spend an entire course or even an entire career focusing on the intricate details of statistical decisions and ideas. However, that is not the focus of BMSS! Instead, we want you to have the statistical grounding necessary to plan your experiments and analyze your data. For biologists, statistics are a tool we can leverage to perform the best possible experiments and test our hypotheses. 

You are ideally going to design an independent research project. That project will likely be driven by an overarching question or hypothesis. At the end of the project, we want you to the right kind of data (and enough data) to assess whether your hypothesis was true or not! That is why experimental design and statistics go hand in hand. This tutorial dives somewhat deeply into sampling design. This may seem boring and unnecessary but if you are able to think about your own experiment in these terms you will make sure you have enough replication to actually assess your hypothesis!


# **3.) The T-test**{.tabset .tabset-pills}

## **T-test theory**

The t-test (or students' t-test) is a basic statistical test used to assess whether or not the means of two groups are different from one another. In this test, the null hypothesis is that the two means are equal (or that there is no difference between the two means). 

A t-test should only be used if the following assumptions are met:
**1.)** the two distributions whose means we are comparing must be **normally distributed** 
**2.)** The variances of the two groups must be **equal**

**Assessing normality**
Method 1: the Shaprio-Wilk Test. If p < 0.05 then the distribution is signficantly different from normal.
```{r}
shapiro.test(mtcars$mpg) #this is normally distributed

shapiro.test(mtcars$cyl) # this is not

shapiro.test(mtcars$hp) #this is not normal (but very close to p=0.05)

shapiro.test(mtcars$wt) #this is normal
```
</br>

Method 2: Visualization

```{r}
ggplot(data=mtcars, aes(mpg))+
  geom_histogram(binwidth = 3) #looks normal

ggplot(data=mtcars, aes(cyl))+
  geom_histogram(binwidth=1) #NOT normal

ggplot(data=mtcars, aes(hp))+
  geom_histogram(binwidth=35) #looks near normal, with some skew 

ggplot(data=mtcars, aes(wt))+
  geom_histogram(binwidth=0.5) #looks normal
```

**Can we ignore either of these 2 assumptions?**

We can if our sample sizes are large. If n is small, we should not ignore this assumption. 
There are alternatives to dealing with normality that we can discuss in the ANOVA section (such as transforming the data)

[For more info on that](https://thestatsgeek.com/2013/09/28/the-t-test-and-robustness-to-non-normality/)

We can also ignore the equal variance requirement if we use the Welch t-test (default in R)


## **A basic T-test in R**

Let's compare two columns in the mtcars data frame

Do the means of mpg and cyl differ?
```{r}

t.test(mtcars$mpg,mtcars$cyl, alternative='two.sided', var.equal=FALSE) #two.sided and var.equal= FALSE are default, so we don't have to list them. BUt, we can also change them (as I will show later)

```

The result of the above t.test shows that p<0.05, so these two means are different. Now, which one is lower? Through looking at the plots we obviosuly know it is cyl, but we can confirm that with a t.test if we want!

**Is the mean of mpg > cyl?**
```{r}
t.test(mtcars$mpg,mtcars$cyl, alternative='greater', var.equal=FALSE) #'greater' here technically asks if the diffrence in means is > zero (in which case, the variable listed first would have a mean higher than the variable listed second)


t.test(mtcars$mpg,mtcars$cyl, alternative='less', var.equal=FALSE) #'less' here technically asks if the diffrence in means is < zero (in which case, the variable listed first would have a mean lower than the variable listed second)

```

As you can see, we can use a t-test to tell us if there is a difference in means between the two groups we are looking at. Which is fine, but not always very useful. Our example here was totally frivolous. We don't need statistics to tell us that the mean of cylinder (which is between 4 and 8) and the mean of mpg (which ranges between <10 and >20) are obviously different. We would already expect that. 

SO, when is a t-test actually useful and when isn't it? Most of the time, this is not the most useful test. That is why we need ANOVA!


# **4.) Analysis of Variance (ANOVA)**{.tabset}

## **ANOVA theory**

Analysis of variance (ANOVA) is a common statistical test used to compare the means of multiple groups. A t-test can only compare the means of 2 groups. ANOVA can compare many groups, making it much more likely to be useful.  

Let's take a look at the theory behind ANOVA.

![ANOVA theory - datanovia.com](/home/jbaumann/BMSS/images/one-way-anova-basics.png)

This figure has 3 groups (blue, yellow, and gray) with means represented by dotted vertical lines. The solid lines are the distributions of each group (essentially, the variation with-in a group). 
This variation within a group is called **residual variance** (Panel B). 
The goal ANOVA is to compare means to see if they differ. The way ANOVA works is that it compares the variance between groups (Panel A) with the residual variance (Panel B). If between group variance is great enough when compared to residual variance then you can conclude that there is some kind of difference between group means (at least 1 group mean differs from the others).

In summary: An ANOVA is a comparison of 2 variance estimates (between group vs within group), that is why we call is analysis of variance even though it really assess differences between means. 

ANOVA relies on something called the **F-statistic** which we can calculate as: variance between groups / variance within groups (residual variance)

## **Assumption of ANOVA** 
ANOVA operates best under the following assumptions: 

**1.) Independence of observations** (each subject belongs to only 1 group, every observation is independent of other observations. There is no relationship or correlation between the observations in each group.)

**2.) No significant outliers exist** 

**3.) Normality:** The data are approximately normally distributed

**4.) Homogeneity of variances** (the variance of the outcome variable should be similar in all groups)

NOTE on assumption violations: 

Independence is the single most important assumption. It should not be violated. If it is, best practice is to run a different (non-parametric) statistical test. 

The outlier assumption is easily handled, as outliers can be removed from data. That said, modern best practices in statistics encourage the user to remove fewer outliers (and only remove extreme outliers). This assumption is often ignored at little cost. 

As with t-tests, if sample sizes are large enough (which they almost always are), the normality assumption can be violated. With small sample sizes you can transform your data to make it normal (w/ log, natural log, or square root).

Homogeneity of variances (Homoscedasticity). This one is complicated. In short,as long as sample sizes (n) are the same (or similar) for your groups, even with low sample sizes you can usually violate this assumption and be fine. Most people DO NOT even test for this assumption (or any of the others), but... it seems possible that any p-values derived from any ANOVA in which assumptions are violated are not that trustworthy. 

There is a lot of argument about how and what the best practices for ANOVA are. Truthfully, I do not fully understand all of this, but I will show you how to do a few tests on your data before doing an ANOVA, just to be careful. I will argue for better statistical practices (that move beyond ANOVA and p-values in general) next week (if we have time). 

## **Testing assumptions**

**1.) Independence.** We need to consider our experimental design to test this one. 

**2.) Outliers**
To look for outliers, we can use a boxplot. Or the identify_outliers() function in the rstatix package.
Here, we see outliers in the 8 cylinder group, but they are not extreme and are likely only outliers due to low n. So we will keep them (this is a judgment call)
```{r}
ggplot(data=mtcars, aes(x=cyl, y=mpg, group=cyl))+
  geom_boxplot() #dots are outliers, we see 2 in the 8 cyl group. But do they really look like outliers???

#make cyl a factor! This matters :)
mtcars$cyl=as.factor(mtcars$cyl)


#identifying outliers using the rstatix package!
mtcars %>% 
  group_by(cyl) %>%
  identify_outliers(mpg) #Here we can see that the 2 points we saw in the boxplot are actually 3 (2 of the outliers have the same mpg). They test as outliers but NOT extreme outliers. So we will not remove them. 
```

**3. & 4.) Normality and Homoscedasticity**

We can test normality and homoscedasticity of the entire anova using check_model() in the Performance package.

```{r warning=FALSE, message=FALSE}
model1<-aov(mpg~cyl, data=mtcars) #this is our ANOVA model! We will learn more about this on the next tab
check_model(model1)

```

We can also assess normality using a Shapiro-Wilk test. If p<0.05 then normality CANNOT be assumed.
```{r}
shapiro_test(residuals(model1)) #here, we have a normal (nearly) distribution
```

To assess normality by group we must use the pipe %>% again!Here, we see that we have normal dist for all 3 groups
```{r}
mtcars %>% 
  group_by(cyl) %>%
  shapiro_test(mpg) #from the rstatix package. Note: the base R version shapiro.test doesn't work here
```

To assess homogeneity of variance, we can use check_model to see it graphically. If we want to see numbers, we can use a levene test. A levene test p<0.05 indicates that there IS a significant difference in variances between the treatment groups. Compare this w/ our plots from check_model(). What do you think? 
```{r}
mtcars %>%
  levene_test(mpg~cyl)
```

Technically, the levene test tells us we cannot do an ANOVA without doing a welch test (which accounts for the different variances). So, we can do the welch test one-way anova. 
Looking at the data in our check_model, we can see pretty clearly that there is a difference in variance among out 3 cylinder groups. So that could be a problem. We can ignore this assumption due to high n (which is done often but isn't always best practice), or we could do a modified version of the ANOVA that accounts for this difference in variation. This is called a welch test. 

## **One-way Anova**

The simplest ANOVA is a one-way ANOVA, which is just an extension of a t-test. T-tests can only compare 2 means, but ANOVA can compare more than 2 groups. The simplest example of an ANOVA is when we have data organized by one grouping variable (hence, one-way). For example, if we want to know how mpg varies by cyl in mtcars, we would use a one-way anova. 
NOTE: if variance differs significantly between our groups, we want to run a welch_anova_test() instead of a one way anove (aov). welch_anova_test() is available in the rstatix package that we loaded above. 
aov() is present in base R.

So, let's plot the data and then do some stats!
```{r}
ggplot(data=mtcars, aes(x=cyl, y=mpg, group=cyl))+
  geom_boxplot() #dots are outliers, we see 2 in the 8 cyl group. But do they really look like outliers???

mtaov<-aov(mpg~cyl, data=mtcars) 
mtaov #normally, an aov() will give us a p-value. Here it recognizes that "estimated effects may be unbalanced". We can find the p-value in summary()
summary(mtaov) #this anova will still give us a p-value

welch_anova_test(mpg~cyl, data=mtcars) #here, we do get a p-value that is <0.05, suggesting that there is an effect of cylinders onf mpg
```

**A second example**
Let's apply those ANOVA skills to a new dataset. 
We will use the dataset called 'iris' that is built into R. It's a simply dataframe that contains attributes of different flowers. It is similar to penguins!

First step: Let's plot some data! I want to assess the effect of species on petal.length

```{r}
head(iris)

ggplot(data=iris, aes(x=Species, y=Petal.Length, group=Species))+
  geom_boxplot(aes(fill=Species))+
  theme_bw()+ #remember this?
  scale_color_npg()+
  scale_fill_npg()

```
</br>
Our plot shows us that there is almost certainly an effect of species on petal length. But, let's check that with an anova. 

Second step: Check ANOVA assumptions. 

Independence: We will assume this is fine. We can't possibly know for sure given the source of the data. 
Outliers: We already checkd visually. Nothing stands out as an extreme outlier. But let's confirm
```{r}
iris %>% 
  group_by(Species) %>%
  identify_outliers(Petal.Length) #No extreme outliers, so let's carry on
```

Normality and Homoscedasticity 

Visual Check
Using check_model() we see that we have a normal distribution (yay) but there is some concern about homogeneity of variance. 
```{r message=FALSE, warning=FALSE}
modeliris<-aov(Petal.Length ~ Species, data=iris)
check_model(modeliris)
```

Numbers check (shapiro for normality, levene for homosced)
Normality is fine, but homoscedasticity assumption is not met, so we need a Welch test
```{r}

#entire model normality
shapiro_test(residuals(modeliris)) #this comes out significant indicating non-normality in the residuals. This is clearly not the case when we look at all of the other data and figures we have, so let's not worry about it. 

#normality of each group
iris %>%
  group_by(Species)%>%
  shapiro_test(Petal.Length) # All come out as normal, note the almost significant p for setosa


#Homogeneity of variance /Homoscedasticity 
iris%>%
  levene_test(Petal.Length~Species) #This is significant, so there is an issue with heterogeneity of variance and that assumption is not met. Thus, we need to do a 

```


Finally, let's run the anova (welch test)
```{r}
#regular anova
aoviris<-aov(Petal.Length~Species, data=iris) 
aoviris #this tells us we may have unbalanced effects
summary(aoviris) #this summary still gives us a p value, so are we really doing something wrong??

#welch anova
welchiris<-welch_anova_test(Petal.Length~Species, data=iris) # We see a significant effect
welchiris

```

## **Two-way Anova**
The reason that ANOVA is so widely used is that it is very flexible. So far, we have learned how to assess whether one variable impacts another. But we can actually assess the impacts of several variables on each other. Here we will talk about a specific case, the two-way ANOVA

Let's take a look at an example Two-Way ANOVA

```{r}
aov1<-aov(mpg~cyl*gear, data=mtcars) #maybe we are unbalanced, we can check assumptions after
summary(aov1) #here we see a p-value and everything else we want for a stats table

```

Check model assumptions
Here, we see that normality is fine, and that there may be some issues w/ variance, but it is probably fine. Multicollinearity is a new figure for us. I will explain it, but in short, it means that our two way anova is testing the effects of 2 variables (cyl and gear) that are correlated. 
```{r message=FALSE, warning=FALSE}
check_model(aov1)
```

What about outliers? 
Let's do a visual and a numerical check
```{r}
ggplot(data=mtcars, aes(x=cyl, y=mpg, color=cyl))+
  geom_boxplot()+
  theme_bw()+
  facet_wrap(~gear)



mtcars %>% 
  group_by(cyl, gear) %>%
  identify_outliers(mpg) #No extreme outliers, so let's carry on
```

Let's revisit our ANOVA and interpret it!
```{r}
summary(aov1)
```

This shows us that there is a significant effect of cylinder on mpg. However, there is no effect of gear of of the interaction of cyl and gear (cyl:gear). This type of ANOVA is called an interactive two-way anova.

When building an ANOVA (or any statistical model) there are two ways to look at multiple factors. We can do additive (+) or interactive (*). Additive tests the effects of each variable seperately. Interactive does the same plus an interaction term. 

Let's look at the difference
```{r}
aov1<-aov(mpg~cyl*gear, data=mtcars)
summary(aov1)

aov2<-aov(mpg~cyl+gear, data=mtcars)
summary(aov2)

```

We see that the p-values are slightly different here and that the additive anova does not include an interaction term. When do you think we might need additive vs. interactive? Does it really matter?
The answer is really dependent upon your question! 

You can use as many terms as you want in an anova, but the more you use the smaller the sample size will get and the harder it will be to interpret. We will talk about best practices in model building next week if we have time. 

For now, we have 1 more thing to learn! We have used ANOVA to determine that there is an effect of cyl on mpg. But, which groups within cylinders are different from each other?

For that, we need post-hoc tests!

## **TUKEY Honestly Significant Difference test (HSD)**

First, let's look at our data. We saw from our ANOVA that there is a significant effect of cyl on mpg
```{r}
ggplot(data=mtcars, aes(x=cyl, y=mpg, fill=cyl))+
  geom_boxplot()+
  theme_bw()+
  scale_color_npg()+
  scale_fill_npg()

aovtest<-aov(mpg~cyl, data=mtcars)
summary(aovtest)
```

Great, we know about that effect. But which number of cylinders gives us the best mpg? It looks like 4 is the answer. But we still don't know! That is really what our hypothesis is most likely the be!

To test that, we need to do a post-hoc test. I like the TUKEY honest significant difference test (which functions like a bnunch of t-tests all at once)

Here is how we do it!
```{r}
aovtest<-aov(mpg~cyl, data=mtcars)
TukeyHSD(aovtest)

```

The Tukey shows us comparisons of each cylinder category with one another. We see that There are significant differences between all 3 groups! This tells us definitely, that MPG is highest with 4 cylinders and lowest with 8. 

***

# **5.) Linear Regression** {.tabset .tabset-pills}

## **Intro to Linear Regression**
There are 3 main types of data distribution. Each type of distribution requires different statistical procedures. 

We have already talked about **Normal Distributions** (aka: bell curves or Gaussian). We can use ANOVA, t-test, and simple linear regression on this type of data. 
We have also talked about how we might be able to use ANOVA approaches on non-normal distributions (with some caveats). 

Below are the three main types of data distributions. </br>

![distributions - ourcodingclub](/home/jbaumann/BMSS/images/data_distributions.PNG)

Note: it is possible for a Poisson distribution to look like a normal distribution. The "Examples" section of the graphic can be very helpful in knowing which type of analysis to choose! 

Here, the graphic shows us that for continuous data (usually approximately normal) we can use ANOVA, lm (linear regression), or mixed effects models (we MIGHT learn this later if there is time).

For count data, including population counts and abundance counts, we usually have a poisson distribution. If that is the case we want to use glm or Generalized Linear Models. Again, the intricacies of these can be really complicated. As such, we **WILL NOT** get to glm and linear mixed effects models as a class this semester. I will be teaching these approaches in my research methods course next term (ecology capstone) and am happy to help you explore them this term if you are interested. For now, I recognize that we may not be able to choose "the best" statistical models for our data, but we can still learn how to use some models test assumptions to understand our limitations. 

For survival data or binomial data like "presence vs absence" we often have a binomial distribution. Here we also want to use glm. 

**Importantly** Model selection and model structure should be driven by your hypothesis. Fitting the "best" model doesn't always allow us to address our hypothesis so we will need to learn how to make judgment calls that allow us to find the best model that still addresses our question!

## **Linear regression theory**

A linear regression essentially compare the correlation of one variable with another. The closer the relationship is to 1:1 (a diagonal line at 45 degrees from the x and y axis) the more correlated the two variables are. Does correlation imply causation? NO, it does not. But this type of analysis driven by hypotheses can help us seek causation/ mechanisms and statistically assess relationships. 

Linear regressions are not very differnt from ANOVA. They gave 4 assumptions: 

1.) Linearity of the data: We assume the relationship between predictor (x) and outcome/dependent variable (y) is approx. linear

2.) Normaliy of residuals: The residual errorr are assumed to be normally distributed

3.) Homogeneity of residual variance (homoscedasticity): We assume residual variance is approx. constant

4.) Independence of residual error terms

**WE NEED TO CHECK THE FOLLOWING WHEN USING LINEAR REGRESSION**

**1.) Non-linearity** 
**2.) Homescedasticity**
**3.) Presence of influential values** (outliers in the outcome(y) variable and extreme values in the predictor (x) variable).

We can do ALL of this very quickly and easily using our old friend the "performance" package :)
We simply need to use:

check_model()

(YAY, finally something easy :) )

Ok, let's apply this theory and test out some basic lm

***

## **Simple linear regression**

In R, we can do a simple linear regression (or linear model) with the function lm() from base R. 

Let's run through an example using iris and test the assumptions. You'll note that this is VERY similar to assumption checking w/ ANOVA.

```{r}
head(iris)
```

FIRST, we need to generate a hypothesis. Let's hypothesize that sepal length will scale positively with sepal width (longer sepals will also be wider). Now, we build out linear regression. In this case, Sepal.Length is our predictor var so it is our x variable. Sepal.Width is our outcome/dependent variable so it is our y variable. In lm() we use this notation lm(y~x) or Y in terms of X

```{r}
irislm<- lm(Sepal.Width ~ Sepal.Length, data=iris)

summary(irislm)
```

***

## **Interpreting your linear regression (R^2 and p-value)**

What does all that output mean? Here is an annotated example to help us learn...

![lm_output - ourcodingclub](/home/jbaumann/BMSS/images/lm_out.png)

At the top of the output we see a reminder or our model structure. This will become more useful when we do more complex models. 

In the middle we see a table of coefficients. This should look kind of like the output from ANOVA. Estimate tells us the mean effect size of each of our variables. This is not overly useful for us this time, so let's talk about this table in the next example. Notably, since x is continous and not categorical, (Intercept) is the values of x when y = 0. 

Further down, we see the values we really care about for a SIMPLE linear model (R-squared and p-value). 
The R-squared tells us how much of the variance is explained by the predictor (Sepal.Length). In other words, this tells us how close to a 1:1 line our model is, where R-squared nearer to 1 is best and closer to 0 is worse (less correlation). We want to look at adjusted R-squared, which we see is VERY low (0.007). We can visualize this with a graph... 

```{r}
ggplot(data=iris, aes(x=Sepal.Length, y=Sepal.Width))+
  geom_point()
```

As we can see, there is NOT a clear linear relationship here. 
Our R-squared value tells us the same thing. 
The p-value is also not <0.05, indicating that there is NOT a significant linear relationship between sepal length and width. 
As such, we can REJECT our hypothesis that longer sepals will be wider. 

To visualize this, we can actually make an lm on our ggplot :)

```{r}
ggplot(data=iris, aes(x=Sepal.Length, y=Sepal.Width))+
  geom_point()+
  geom_smooth(method=lm)+
  theme_bw()
```

***

## **checking assumptions**

We can just use check_model in the "performance" package to quickly look at them...

```{r warning=FALSE, message=FALSE}
check_model(irislm)
```

Here we see that we have nearly normal residuals, a nearly normal distribution, SOME concerns about homoscedasticity (but overall, it seems mostly fine), and reasonable homogeneity of variance. 
The last plot (Cook's Distance) shows us influential observations. Unless Cook's distance is >1 (x-axis) we don't need to worry. So here, we are good! 
In short, our model was acceptable so we can trust what we saw!

*** 
## **Linear models with categorical variables**

Let's make this a little more interesting... 

We can look at mtcars this time... 
```{r}
head(mtcars)
```


Now, I want to hypothesize that there will be no effect of cylinder on horsepower (this is called a "null hypothesis"). We've seen similar hypothesis before in our ANOVA. 

First, let's make cylinder a factor and plot a boxplot so we can see whether there may be a trend here... 

```{r}
mtcars$cyl1=as.factor(mtcars$cyl)

ggplot(mtcars, aes(x=cyl1, y=hp))+
         geom_boxplot()+
         theme_bw()

```


I think it is safe to say we see what we might suspect to be a linear(ish) relationship between cyl and hp, where hp increases as cyl increases. What do you think?

Now, let's do some stats on this. 

```{r}
lmhp<-lm(hp~cyl1, data = mtcars)
summary(lmhp)
```

This time we used a categorical x variable, which makes things a little more interesting. In the coefficients table this time we see cyl = 6 and cyl =8 represented as well as "intercept." R takes the catergorical variables and places them in alpha numeric order in these tables. So "intercept" is actually cyl=4. The "estimate" tells us the effect size of each category relative to "intercept." SO, the mean of cyl=4 should be 82.64 (check the boxplot above to confirm). The mean of cyl=6 is not 39.65, but is actually 39.65 higher than mean of cyl=4 (82.64 + 39.65 = 132.29, which checks out). The p-values associated with each of the coefficients test the null hypothesis that each coefficient has no effect. A p <0.05 indicates that the coefficient is likely to be meaningful in the model (changes in the predictor's value are related to changes in the reponse value).

Further down, we see an R-squared of nearly 0.70, which is very good evidence of a linear relationship (70% of the variance in y can be explained by x!). The p-value is very nearly 0.00, which indicates a significant linear correlation. 

**Let's check assumptions!**

```{r meesage=FALSE, warning=FALSE}
check_model(lmhp)
```


Here we see some concern about Homoscedasticity and homogeneity of variance. We can probably still assume our model is reliable, but we may want to be careful. We learned ways to numerically assess this last week, but again, with high enough sample size, this won't be an issue. Here, I would suggest that n is too small, so if this were a real statistical test we would have limitations to discuss. 


Remember our hypothesis (null) was: "There will be no effect of cylinder on horsepower." We are able to reject this null hypothesis and suggest that indeed horsepower increases as cylinder increases. We might also add caveats that homoscedasticity was not confirmed due to low sample size, but the result seems clear enough that this likely doesn't matter. 
***

## **ONE MORE TEST of lm --> more complexity :)**

let's go back to iris 
```{r}
head(iris)
```

While looking at these data I might think that species impacts the relationsip between petal length and petal width. I might generate the following null hypothesis: 
"Species has no effect on the relationship between petal length and petal width"

Let's visualize this...

```{r}
ggplot(data=iris, aes(x=Petal.Width, y=Petal.Length, group=Species, color=Species))+
  geom_point()+
  facet_wrap(~Species, scales='free')+ #scales='free' allows the x and y ranges to vary by facet. This lets us have a better look at the comparison between petal width and length for each species
  theme_bw()+
  scale_color_aaas()
```

What we see here is unclear. Let's try to put an lm() layer on this and then look at some stats. 

```{r}
ggplot(data=iris, aes(x=Petal.Width, y=Petal.Length, color=Species))+
  geom_point()+
  facet_wrap(~Species, scales='free')+
  geom_smooth(method=lm)+
  theme_bw()+
  scale_color_aaas()

ggplot(data=iris, aes(x=Petal.Width, y=Petal.Length, color=Species))+
  geom_point()+
  #facet_wrap(~Species, scales='free')+
  geom_smooth(method=lm)+
  theme_bw()+
  scale_color_aaas()
```

This visualization shows us that there are probably positive linear correlation between petal width and length but that the exact correlation varies by species. 

Let's test out an interactive lm (using '+' (additive) will give us insights into how intercepts change, using interactive will also give us insights into how slopes change!) 

```{r}
specieslm<-lm(Petal.Length ~ Petal.Width * Species, data=iris)
summary(specieslm)
```

Here, we see a more complex coefficients table. 
The "intercept" in this case is the value of y (petal length) when x (petal width) = 0 and species = setosa. This is not necessarily the most useful thing for us to look at. That said, we see significant p-values for all variables, indicating that species does impact our linear models. 
Petal.Width : 'species name' shows us differences in slopes between the species. Here we see that slope of verisolor differs from slope of setosa, but slope of virginica does not. 

The R2 of our overall model is 0.95!, and p is ~ 0.0, so we have a clearly significant relationship here. 

In summary, what we see is that there is a strong (0.95) positive (from the graph) correlation between petal length (y) and petal width (x) and that species of iris has a significant effect on that correlation (slope of the lines can differ by species). Our model shows us that each species still shows a strongly positive linear correlation but that there are significant differences between species. 


Finally, let's test assumptions

```{r message=FALSE, warning=FALSE}
check_model(specieslm)
```

Right away, the first panel here shows us that we have HIGH multicollinearity, which is something we have not talked about yet. This essentially means that the predictors we used are highly correlated. This usually indicates that the variables are NOT independent of one another. IF we get into more complex modelling we can worry about this, but a way to eliminatae this would be to pull the effect of species out of our model. However, doing this, while it might make the "best" statistical model, will no longer allow us to address our hypothesis, so we need to make a judgement call here. This time, I elect to keep everything in the model. 

Luckily, we see nice data for normality of residuals, so that's great. 
Homoscedasticity actually looks decent for once, and there are no clear influential observations. In short, we really did ok here. We can assume this model is reliable. 

Now, let's go back to our null hypothesis: "Species has no effect on the relationship between petal length and petal width"

We can reject this null hypothesis as we had a very low p-value on our model. We can instead say that there is a significant positive linear correlation between petal length and petal width that varies in slope based on species. To take this even further, we can say that the slopes of setosa and virginica do not differ but the slopes of setosa and versicolor do! By looking at our plot, we can see that the correlation is stronger in versicolor than it is in the other two species!

***

# **6.) Applying what we've learned**
1.) Grab some data and generate a hypothesis about it (a difference between groups? No differences? You can choose!)
2.) calculate means and error ranges and plot
3.) Run a suitable ANOVA (and Tukey if necessary) to match your graph and hypothesis
4.) Evaluate your statistical test. Were assumptions met? Can they be ignored? What is the outcome of the statistical test? Was your hypothesis incorrect?
5.) Apply the same procedure for linear regression: develop a hypothesis, plot (w/ regression line) and carry our the linear regression test. Evaluate the linear regression. What does it tell you about your hypothesis?







